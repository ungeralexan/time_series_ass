\documentclass[a4paper,12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{hyperref}



\begin{document}
\section{The Data Generating Process}
For the purpose of our analysis we will first introduce our true ARMA(1,1) process which will be assumed to have created the time series data.
This ARMA(1,1) it will entail exactly one autoregressive part and one moving average part. Whereas the autoregressive part will be later of great importance to determine if the process is stationary.
The data generating process will be described in the following way.
\begin{equation}
    y_t = c + \phi y_{t-1} + \theta \epsilon_{t-1} + \epsilon_t
\end{equation}
It is important to mention that the innovations in our process will be t distributed $\epsilon_t \sim t(\nu) $. Moreover, our process will be defined as having the following true parameters:
\begin{equation}
    y_t = 2 + 0.95y_{t-1} + 0.25\epsilon_{t-1} + \epsilon_t
\end{equation}
The innovation terms will follow the t distribution and we will define $\nu$ =4, meaning four degrees of freedom. Hence, the innovation distribution has heavier tails than a Gaussian distribution.
As mentioned in Hamilton to now assess whether our process is stationary we will check whether the autoregressive coefficient ($\phi$) is smaller then one in absolute terms.
For our case we can clearly conclude that $|0.95| <= 1$. 
Therefore, our true process is stationary. We could also assess invertibility but we will neglect this for the time being.
As our process is stationary ($\phi<1$) the coefficients our coefficients are absolutely summable and can define the process in a MA infinity representation which would yield after Hamilton:
\begin{equation}
    \mu + \Phi(L)\epsilon_t
\end{equation}
The mean now can be calculated after Hamilton
\begin{equation}
\frac{c}{1-\phi} = \frac{2}{1-0.95} = 40
\end{equation}
Our expected value is 40 and can be easily calculated as it only depends on the autoregressive parameter, whose value is already known to us.

\section{Realization of the process}
Using the data generating process we have defined in (2) we will now create one realization of the process.
This realization of the process was plotted and can be seen in picture(1).
It is important to mention that for the following graph we simulated 800 observations but due to dependencies in the first 50 observations we decided to discard the first 50 observations as burn in phase.
Moreover, we selected the starting values for our simulation of the realization of the process wich is the same as the expected value of the process, displayed in (4).
A first glance of the graph shows how the data fluctuates around the expected value of 40 which is represented through the dashed red line and subsequently remains close to the theorietical mean which represents a stationary behaviour. 
Furthermore, the data does not display any clear upward or downward moving trend; the process just randomly oscillates around the mean. Additionally, a slight persistance in the data can be seen, as the data entails smoother and less eratic transitions between the successive values. Around the 400 and 500 values region the data eshibts abrupt changes and outliers which can be due to the heavier tails of the t distribution that we defined for the process. Finally the moving average coefficient contributes to short term corrections, which introduce additional randomness to the series but are less dominant compared to the autoregressive component.
In the next sections we will work with the realization of the process asuming that we dont know anything about the data generating process and therefore try to estimate the true process

\section{Dicky Fuller test statistic}
In real world applications researchers mostly face the problem of not knowing about the true data generating process, meaning about which process actaully generated the time series they are looking at and therefore have trouble to say whether certain moments of the series can be extrapolated for the true population moments. 
In this paper we will from now one assuming that we dont know the true process that generated the data and therefore, we will try to conclude from this time series whether the simulated realization is stationary, meaning that the moments of the series are constant over time. Stationarity in this purpose is crucial, as only with stationarity we can also assess whether we encounter erdogicty. 
We want to have erdogicty as in this case a law of large numbers applies and the time series averages will converge either in probability or almost surely towards their expectation counterparts. 
In order to asess whether our data is stationariyt we will use the Dicky Fuller test, which 
To assess stationarity we will now be using the dicky fuller test, which explicitly based on different cases tests whether the process has an unit root or not.
\subsection{Selecting the case}
In order to perform a Dicky Fuller test we will have to decide for a case in order to estimate the model.
It is important to choose as a case which will later yield the alternative hypothesis enough power to reject a false nullhypothesis meaning a rejecting when the process does not have an unit root.
By inspecting the plot we see as mentioned above how the data randomly fluctuates over a nonzero mean which in this case is the value of 40. Therefore we will choose the second case of the dicky fuller test. We will neglect the first case as we there would expect the data to fluctuate around a zero mean which is not the case in our plot and therefore we would provide our stationary alternative with not enough power to reject a false nullhypothesis as the stationary true could seldom produce a fluctuating realization around a nonzero null. Furthermore, we also discared case three and case four. Case 3 will be discarded as we first do not observe a certain druft in the data and moreover, this case yields way to less power to the stationary alternative. Moreover, the fourth case will also be discareded as we mentioned above there is clearly no upward or downward moving trend in the data observable and therefore no reason to decide on the fourth case of the dicky fuller test.
Based on this argumentation the second case of the Dicky Fuller test perfectly aligns with the observations that are depicted in the plot.
\subsection{Estimate the model for the test}
For the second case of the Dicky Fuller test as for Hamilton we would expect the true model under the Nullhypothesis as 
\begin{equation}
    y_t = y_{t-1} + ut
\end{equation}
therefore our parameter $\rho$, which is the parameter of interest would equal the value one. On the other hand the model defining the stationary alternative hypothesis would look the following way
\begin{equation}
    y_t = \alpha + \rho y_{t-1} + ut
\end{equation}
Whereas the $\alpha$ must be bigger then zero and our $\rho <= 1$ in absolute terms. 
Again it can be perfectly seen how this case yields enough power to the alternative hypothesis as the model in the alternative could have created a nonzero mean reverting process as long as the paramters would be defined as above.
For the purpose of conducting such as Dicky Fuller test for the second case we will now estimate our paramters $\hat{\alpha}, \hat{\rho}$ using the model define in equation (6).
\\
After estimating the paramter values using the Ordinary Least Squares approach we obatin $\hat{\rho} = 0.96$ and for $\hat{\alpha}$ = 1.46. The corresponding standard errors are 0.01 for $s.e.(\hat{\rho})$ and 0.4 for $s.e.(\hat{\alpha})$. If we would compare the  paramter estimates with our true parameter values in the second equation, we see that the paramter ($\hat{\rho}$) is very close to the true parameter of $\phi$. On the other hand the value for the constant $\alpha$ differs more noticeably from the true intercept.
This discrepancy is also reflected in the standard errors: the small standard error of $\hat{\rho}$ suggests that the data are very informative about the persistence of the process. By contrast, the higher standard error of $\hat{\alpha}$ arises because, in a near-unit-root setting of $\rho$, even small changes in $\rho$ can can greatly affect the implied mean of the process.Consequently, the intercept estimate is more vulnerable to finite-sample fluctuations and to the influence of the heavy-tailed errors.
Hence, we observe a higher standard error for $\hat{\alpha}$ then for $\hat{\rho}$.

\subsection{Test statistic}
We will now  test for an unit root in $y_t$ using the second case of the Dicky Fuller test. 
We therefore define our Nullhypothesis as 
$$H0 : \rho = 1 \, \text{(unit root)}$$
and the alternative hypothesis as 
$$H1: \rho \neq 1 \, \text{(stationary alternative)}$$
To test whether the process entails an unit root I will define the complete test statistic as the following
$$t-stat: \frac{0.96-1}{0.01}$$
It is important to mention that the Test statistic under the Null is not normally distributed as we are now in the case where we asumme an unit root. Nevertheless, we know that the distribution of the test statistic under the null is nonstandard but as well nuisance parameter free. For our critical values we now cannot use the conventional 1.96 from the normal distribution. Instead we will compare it  against the critical values of the from the Dicky Fuller table.
 For over 500 observations, a zero trend case, and a 5\% significance level, the critical value is approximately -2.86.
The resulting value of our test statistic is -3.66 and therfore is less then -2.86, and therefore we do not fail to reject the nullhypothesis of an unit root for the significance level of 5 percent.
Having established that our process does not contain a unit root, we next estimate its parameters via Maximum Likelihood (ML). 



\section{Conditional Maximum Likelihood estimation}
We now use Conditional Maximum Likelihood in order to estimate the true paramters that created the series. 
Unlike exact Maximum Likelihood, CMLE assumes the first observation, $y_1$, is deterministic and maximizes the likelihood conditioned on this initial value see Hamilton p(122).
This approach is computationally efficient, as noted by Hamilton (p. 123), and under the assumption of a large sample size, the first observation makes a negligible contribution to the total likelihood. 
Given the stationarity condition $\phi < 1$ (proven in the previous section), the Maximum Likelihood Estimators (MLE) and Conditional Maximum Likelihood Estimators (CMLE) share the same large-sample distribution.
For the purpose of conditional maximum likelihood we again as in the prior section assume that the we dont know the true paramters that created the time series.

By maximizing the conditional log-likelihood function with respect to the parameters, we aim to identify the values most likely to have generated the observed data. Additionally, we assume that the process innovations follow a $t$-distribution:
 \begin{equation}
     \epsilon_t \sim t(\nu) 
 \end{equation}
 where $\nu$ denotes the degrees of freedom.
The choice of a $t$-distribution is particularly advantageous due to its ability to model heavy-tailed errors, which are often encountered in real-world data. Unlike Ordinary Least Squares (OLS), which assumes normally distributed errors and minimizes the sum of squared residuals, the correctly defined CMLE accounts for the specific distributional properties of the innovations.
In order two get the maximum likelihood estimates this paper will follow two steps. First we will propose two distinct parameter specifications and compare their respective log-likelihood values. This comparison allows us to determine which specification offers a better fit to the data, thereby providing insights into the underlying dynamics of the process.
Later in the next section we will try to estimate the paramters by numerical search which in this context will be minimizing the log likelihood.
 \subsection{Estimate the Likelihood}
In order to implement our first approach, we compute the conditional log-likelihood contributions for each time step. 
The innovations, $\epsilon_t$, are calculated recursively based on the ARMA(1,1) model:
\begin{equation}
    \epsilon_t = y_t - c -\phi y_{t-1} - \theta \epsilon_{t-1}
\end{equation}
In order to stabilize the maximization of the likelihood by taking the sum of densities instead of the multiplication of densities by taking the logartihm.
The conditional log likelihood contributions are then given by
 \begin{equation}
      l_{t} =  (ln \Gamma(\frac{\nu +1}{2})- ln(\sqrt{\nu \pi} * \Gamma(\frac{\nu}{2})) - \frac{\nu+1}{2}ln(1+\frac{\epsilon_t^{2}}{\nu}))
 \end{equation}
The total conditional log likelihood is then obtained by summing these contributions over all time steps .
\begin{equation}
    L_{total} = \sum_{t=2}^{T} l_t
\end{equation}
We compute the conditional log-likelihood for two parameter specifications:
\begin{itemize}
    \item \textbf{Case (a):} $c = 2, \phi = 0.95, \theta = 0.25, \nu = 4$ (the true parameters used to generate the data),
    \item \textbf{Case (b):} $c = 1.5, \phi = 0.75, \theta = 0.5, \nu = 6$ (an alternative set of parameters).
\end{itemize}
The computed condtional log-likelihood values for each case are summarized in Table~\ref{tab:results}.
As shown in Table~\ref{tab:results}, the conditional log-likelihood value for Case (a) (the true parameter specification) is significantly higher (less negative) than that for Case (b). This indicates that the true parameters provide a better fit to the observed series compared to the alternative specification.
In the context of log-likelihood, "less negative" values are critical because the log-likelihood is derived from the natural logarithm of the joint probability density of the data given the parameters. Higher (or less negative) log-likelihood values correspond to a greater probability density for the observed data, indicating that the specified parameters are more likely to have generated the series.
Therefore, the lower (more negative) log-likelihood value for Case (b) reflects a poorer fit to the data.
The deviations in the AR and MA coefficients, as well as the degree of freedom parameter $\nu$, result in a suboptimal representation of the data-generating process. 
This discrepancy highlights the sensitivity of CMLE to parameter specification. By explicitly accounting for the heavy-tailed $t$-distributed innovations, CMLE effectively distinguishes between parameter sets and favors those that align closely with the true data-generating process.
\subsection{Estimate the paramters with numerical minimization}
In this section we will now try to estimate the parameters $c, \phi, \theta,\nu$ by maximizing the conditional log-likelihood function. To achieve this, we adopt a numerical optimization approach.
It is worth noting that instead of directly maximizing the log-likelihood, we minimize the negative log-likelihood. These two approaches are mathematically equivalent, as minimizing the negative of a function is the same as maximizing the original function. This equivalence stems from the monotonic nature of the negative operation, which preserves the order of the optimization objective.
To adapt our method, we refine the log-likelihood function introduced earlier (Equation 10) to return its negative value. This ensures compatibility with standard numerical optimization algorithms, which are designed to find the minimum of an objective function. In order to minimze the negative of the function we used the 'fminsearch' algorithm from matlab which is also known as derivate free optimization. 
We moreover, specified a Hessian-based covariance matrix to quantify parameter uncertainty and initialized the estimation procedure with the following values c = 1.5, $\phi$ = 0.75, $\theta$ = 0.5, $\nu$ = 5.
The results of the numerical optimiuation can be found in Table~\ref{tab:results2}.
The results of the numerical optimization, presented in Table~\ref{tab:results2}, indicate that the Conditional Maximum Likelihood Estimation (CMLE) approach successfully estimated the parameters of the ARMA(1,1) process. The estimated values for \(c\), \(\phi\), \(\theta\), and \(\nu\) are close to their true values, demonstrating the robustness and accuracy of the method.

Specifically:
\begin{itemize}
    \item The estimated constant term, \(\hat{c} = 2.2456\), closely approximates the true value of \(c = 2\).
    \item The autoregressive coefficient, \(\hat{\phi} = 0.9446\), is near the true value of \(\phi = 0.95\), indicating accurate capture of the process's dependency structure.
    \item The moving average coefficient, \(\hat{\theta} = 0.2834\), is similarly aligned with the true value of \(\theta = 0.25\).
    \item The degrees of freedom for the \(t\)-distributed innovations, \(\hat{\nu} = 3.4515\), suggests that the estimation effectively captures the heavy-tailed nature of the errors, with only minor deviations from the true \(\nu = 4\).
\end{itemize}

The standard errors derived from the Hessian-based covariance matrix indicate reasonable precision for all parameter estimates, with relatively small variances. The minimized negative log-likelihood value of 1386.5 confirms the plausibility of the parameter estimates, while the small gradient magnitude at the optimum (\(\|\nabla \hat{L}_{\text{neg}}\| \approx 10^{-5}\)) and a successful optimization exit flag further validate the convergence of the procedure. It is worth noting that the minimized negative log-likelihood value of 1386.5 is slightly better than the positive log-likelihood value of the true parameter process, which was \(-1388.32\). This small improvement could arise due to the flexibility of the numerical optimization in finding parameter estimates that fit the observed realization of the series slightly better than the true population parameters. This phenomenon is possible because the observed data is a finite realization of the process, and the optimization procedure tailors the estimates specifically to this realization, potentially compensating for random variations in the sample.
In conclusion, the CMLE approach, coupled with numerical optimization, provides a reliable and efficient framework for parameter estimation in ARMA(1,1) processes. The results demonstrate the practical utility of this methodology in accurately recovering the underlying parameters of a data-generating process.

\subsection{Confidence intervalls}
After estimating the paramters of the ARMA(1,1) process using numerical optimization we now want to assess the precission of the estimates by calculating the standard errors and constructing 95\% confidence intervals.
Confidence intervals are essential for determining the range within which the true parameter values are likely to lie, given the data and model assumptions.The 95\% confidence intervals are constructed based on the standard errors derived from the Hessian-based covariance matrix. These intervals provide a statistical test for the null hypothesis that the true parameter values are equal to specific estimates. 
For values within the confidence interval, we fail to reject the null hypothesis at the 5\% significance level, suggesting that these values are consistent with the observed data. This will help us to evaluate the reliability and uncertainty of the paramter estimates.
Due to the asymptotic normality assumption of the Maximum Likelihood estimates we will treat the paramters as normally distributes even though our innovations are t distributed, this is due to our large sample size. 
The results for our confidence intervalls which are displayed in ~\ref{tab:ci_results} and indicate that first the estimate of \(c = 2.2312\) falls within the confidence interval \([1.4271, 3.0352]\), underscoring that we could not reject the nullhypothesis that our estimate is the true value at least for 5 percent signifcance. On the other hand the interval is relatively wide compared to the other parameters, indicating higher uncertainty in the estimate. The estimate of \(\phi = 0.9452\) is highly precise, with a narrow confidence interval \([0.9255, 0.9648]\). The confidnece intervall moreover, excludes one and therefore confirms the stationarity of the process. The estimate of \(\theta = 0.2837\) lies within the interval \([0.2231, 0.3443]\), demonstrating moderate precision, still we could not reject the nullhypothesis indicating that this value is the true paramter value.  The estimate of \(\nu = 3.4691\) suggests that the error distribution has heavy tails, as the interval \([2.7630, 4.1752]\) excludes values consistent with normality (\(\nu \to \infty\)).
Overall, the results confirm the robustness of the parameter estimates, with the confidence intervals supporting their statistical significance and alignment with the underlying process characteristics. 


\section{Hypothesis testing}
With the parameter estimates and the confidence intervalls established, we now focus on hypothesis testing to assess specific claims about the paramters of our ARMA(1,1) model. 
We therefore will consider the null hypothesis $H0: \phi =0.8$, testing whether the autoregressive coefficient is significantly different from this hypothesized value.  Using the t-test and a significance level of 5\%, we aim to evaluate whether the data provides sufficient evidence to reject this hypothesis.
To test whether the autoregressive coefficient \(\phi\) is significantly different from 0.8, we perform a t-test using the following hypotheses:

\[
H_0: \phi = 0.8 \quad \text{(The null hypothesis)}
\]
\[
H_1: \phi \neq 0.8 \quad \text{(The alternative hypothesis)}
\]

The test statistic for the t-test is calculated as:
\[
t = \frac{\hat{\phi} - \phi_0}{\text{SE}(\hat{\phi})},
\]
where:
\begin{itemize}
    \item \( \hat{\phi} \) is the estimated value of the parameter,
    \item \( \phi_0 = 0.8 \) is the hypothesized value under \( H_0 \),
    \item \( \text{SE}(\hat{\phi}) \) is the standard error of the estimated parameter.
\end{itemize}

Given that the standard error of \( \hat{\phi} \) is \( \text{SE}(\hat{\phi}) = 0.0100 \), and the estimated value is \( \hat{\phi} = 0.9452 \), the t-statistic becomes:
\[
t = \frac{0.9452 - 0.8}{0.0100} = \frac{0.1452}{0.0100} = 14.52.
\]
For a two-tailed t-test at the 5\% significance level (\(\alpha = 0.05\)), the critical value is:
\[
t_{\text{crit}} = \pm 1.96,
\]
assuming the sample size is large enough for the test statistic to follow a standard normal distribution.
The calculated t-statistic is \( t = 14.52 \), which is much greater than the critical value of \( t_{\text{crit}} = 1.96 \). Therefore, we do not fail to reject the null hypothesis \( H_0: \phi = 0.8 \) at the 5\% significance level.
This again underlines a stronger persistence in the time series than would be expected under the null hypothesis.


\subsection{Two-Sided p-Value}
We moreover, calculated the two-sided p-value, which quantifies the probability of observing a test statistic as extreme as \( t = 14.4996 \), assuming \( H_0 \) is true. The p-value is computed as:
\[
p = 2 \cdot (1 - \Phi(|t|)),
\]
where \( \Phi(|t|) \) is the cumulative distribution function (CDF) of the standard normal distribution. Substituting \( t = 14.4996 \), we find:
\[
p = 2 \cdot (1 - \Phi(14.4996)) \approx 0.000000.
\]
Since \( p < 0.05 \), we do not fail to reject the null hypothesis \( H_0 \) at the 5\% significance level. This result is consistent with the conclusion of the t-test, further reinforcing the finding that the estimated autoregressive coefficient \(\phi\) is significantly different from 0.8.
The extremely low p-value (\( p \approx 0.000000 \)) indicates that the likelihood of observing such a large test statistic under the null hypothesis is essentially zero. This provides strong evidence against \( H_0 \), suggesting that the persistence in the time series, as captured by \(\phi\), is significantly greater than the hypothesized value of 0.8.

\subsection{Sample size}
To assess the impact of sample size on paramter estimation and hypothesis testing, we increase the number of observations for the ARMA(1,1) process from T=800 to T=50000. By repeating the estimation of the paramters with conditional maximum likelihood, computing the confidence intervalls and finally test the hypothesis, we evaluate how a significantly larger sample influences the precision of parameter estimates, the results of hypothesis tests, and the computation of the covariance matrix.
The parameter estimates obtained with T=50,000 observations are presented in Table~\ref{tab:results_large_sample}.
For this task we used the same starting values as for the first approach, and moreover we again minimized the negative of condtional log likelihood using a computational approach. 
Table~\ref{tab:param_estimates_comparison} shows the comparison of the paramter estimates for different number of observations. We can clearly see how increasing the sample size improves both accuracy and precision of the paramter estimates, as evidenced by estimates closer to the true values for all four paramters. 
Furtheremore, across all parameters, standard errors decrease dramatically with the increase in sample size, reflecting improved precision. For instance: For c the standard error decreases from 0.40 to 0.05. 
We moreover can observe how the negative log likelihood increases from 386.5 (T=800) to 84,302 (T=50,000). This increase is expected due to the larger sample size, as the log-likelihood scales with the number of observations.
Finally Table~\ref{tab:cov_matrix_comparison} shows how the covarinace matrix shrinks consistently, indicating a reduced uncertainty in the paramter estimates as T increases. 
The increase in the number of observations from 800 to 50000 highlights the importance of the sample size, as it not only reduces the estimations the estimation errors but also provides reliable measures of uncertainty.

\section{Conditional Maximum Likelihood}
It is common in practice that we dont know the underlying distribution of the stochastic process.
This makes it challenging to specify the correct conditional density function, and often results in the misspecification of the conditional denisty function.
Such misspecification has implications for the parameter estimates: while some parameter estimates may remain consistent, others may be affected by the misspecification.
In cases of misspecified densities, we transition from the framework of Maximum Likelihood Estimation (MLE) to Quasi-Maximum Likelihood Estimation (QMLE).
Under QMLE, the parameter estimates are obtained as though the specified conditional density were correct, even when it is not. 
In the following, we deliberately assume an incorrect conditional density by selecting a Gaussian distribution for the innovations, even though the true underlying innovations are \( t \)-distributed.
This careful misspecification involves selecting a conditional density from the same family of distributions—in this case, the exponential family.
By doing so, we aim to retain some desirable properties of the estimators, such as consistency and asymptotic normality, for certain parameters.
However, the misspecification precludes achieving efficiency for the parameter estimates, meaning that the standard errors will differ, leading to different confidence intervals.
Specifically we will for the following assume that our innovation $\epsilon_t$ are Gaussian instead of t distributed.
 \begin{equation}
     \epsilon_t \sim N(0,\sigma^2) 
 \end{equation}
This assumption introduces a misspecification, as the true innovations are \( t \)-distributed. However, by carefully selecting a distribution from the same family (exponential family in this case), we hope to maintain consistency for some parameter estimates. 
 Our new conditional log likelihood function now looks the following
 \begin{equation}
     ln L = \sum_{t=2}^{T} ln (\frac{1}{\sqrt{2\pi \sigma^2}} exp(\frac{(y_t -c -\phi y_{t-1} - \theta \epsilon_{t-1})^2}{-2 \sigma^2} ))
 \end{equation}
Despite the misspecification, we still expect consistency for some of our parameter estimates, as the QMLE framework is robust under mild conditions. However, efficiency is lost, resulting in different standard errors and confidence intervals compared to those obtained under the correct distributional assumption.
The QML results based on the Gaussian assumption for the innovations are presented in Table~\ref{tab:qml_results}. It is clearly visible that while some paramter specifications remain robust especially for $\phi$, the misspecification introduces slight deviations for other paramter values such as c or $\theta$.
c for instance is slightly overestimated compared to the true value, likely due to the Gaussian misspecification. The same holds for $\theta$, which is slightly overestimated. On the other hand the estimate for $\phi$ is close to the true value. The fourth parameter cant be interpreted as it now measures the variance instead of the degrees of freedom.
Moreover, The negative log-likelihood at the optimum is 1434.4236, higher than the 1386.5 under the correct t-distribution.
This reflects the poorer fit of the Gaussian assumption compared to the true t-distribution, as the Gaussian density cannot account for the heavy tails of the innovations. The diagonal elements for the variance covariance matrix suggest relatively precise estimates for $\phi$ and $\theta$ but higher uncertainty for c and $\nu$.
Having conducted Quasi-Conditional Maximum Likelihood (QML) estimation under the Gaussian assumption, we now report the estimated parameters, their standard errors, and the 95\% confidence intervals.  This allows us to directly compare the QML findings with the Maximum Likelihood (ML) findings, where the true t-distribution of the innovations was correctly specified. By contrasting the two sets of results, we aim to evaluate the impact of the misspecified density on the parameter estimates and their precision.
By observing the standard errors of the Quasi Maximum Likelihood estimation we see that they are generally larger than the conditonal Maximum Likelihood standard errors from the section before. This loss seems to be consistent due to the misspecification.
Furtheremore, The 95\% confidence intervals for the QML estimates are wider than those for the ML estimates, reflecting reduced precision. However, the intervals still include plausible values, showing that the QML estimates retain some reliability. Especilly as the estimate for instance of $\phi$ lies in the confidence intervall we could not reject the nullhypothesis that for this paramter estimate.


\section{Simulation}
In order to better understand the variability of the paramter estimates under Conditional Maximum Likelihood (ML) and Conditional Quasi Maximum Lieklihood (QML), we will simulate the 2500 realizations of the ARMA(1,1) process using the true paramter values, the length and burn-in phase we have used in the first section, which will yield us a matrix of 2500 ensemble realizations.
For each realization, we estimate the parameters using both ML (correct t-distribution assumption) and QML (Gaussian assumption).
We will then use the parameter estimates to analyze the distribution of the estimates across all realizations. 
Additionally, recovering the covariance matrices and extracting standard errors allows us to evaluate the precision of the estimates for both methods under repeated sampling. We hope to get insights into the robustness and precission of ML and QML approaches under varying realizations of the stochastic process.
This repeated simulation will also help comparing the ML and QML results for different realizations of the process, and furtheremore, asessing the impact of missspecifying the distribution of the Quasi Maximum Likelihood estimation in the paramter estimates and standard errors.
For the results of our estimates we will provide the visualizations in Figure(2) where we display the kernel density plots for the parameter estimates for $c, \phi,\theta$obtained using both Maximum Likelihood (ML) and Quasi-Maximum Likelihood (QML). 
Kernel density plots here provide a smooth continous representation of the distribution of parameter estimates, allowing for a visual comparison of their behavior under ML and QML frameworks.
The first plot displays the estimation of the intercept term $c$ over 2500 realizations using the ML and QML frameworks. It clearly shows how the ML density (blue) is centered slightly closer to the true value of c=2 compared to the QML density (red). Moreover, the red density shows a greater spread , reflecting increased variability due to the misspecified Gaussian assumption.
We can conclude that for the 2500 simulations the ML estimation provides a more precise and accurate estimate of c then the QML framework. 
The second plot shows the estimation of the $\phi$ term. Different to the $c$ paramter here Both ML and QML densities are tightly centered near $\phi$ = 0.95, indicating robustness of this parameter to misspecification.The spread of QML estimates is marginally wider, showing a slight loss of efficiency compared to ML. We could conclude that the $\phi$ term is robust to misspecification, with both ML and QML yielding highly accurate estimates.
Finally for the $\theta$ estimate we again see that both densities are tightly centered near the $\theta$ = 0.25. The ML density is slightly more peaked and centered closer to the true value compared to the QML density. The QML density is more spread out, indicating greater variability under the misspecified Gaussian assumption.
For all the three paramters the ML estimates provide more precise estimates, especially the $c$ paramter estimates of the QML estimation shows a greater variability.
The results underscore the loss of efficiency when misspecifying the condtional densities when using the Quasi Maximum Likelihood framework, nevertheless we are still able to get close estimates for parameters like $\phi$ when carefully selcting the density from a familiar distribution.

\subsection{Ci and se}
Finally we want to emphasize again on the properties of the condtional Maximum Likelihood approach and therefore we will now compute the confidence intervalls for our ML estimates of the 2500 realizations based on the prior calculates standard errors.
By comparing these intervals to the true parameter values used to simulate the ARMA(1,1) process, we can determine the proportion of intervals that successfully contain the true values. The results for the confidence intervalls can be found in Table~\ref{tab:coverage_results}. Overall it is to say that the coverage results are very close to the 95\% level, underscoring that the conditional Ml estimation procedure is providing reliable confidence intervals for all parameters. Especially the coverage for
the intercept $c$ (94.72\%) is very close to the 95\% level. 
The coverage for $\phi$ is also close to the 95\% level, a finding that is underscored by the Kernel density plots in figure 2, where the estimated paramters for $\phi$ often laid close to the true parameter value. 
For the MA coefficient the coverage is slightly lower (94,28\%) the for the other two paramters and for the $\nu$ parameter the coverage slightly exceeds the nominal 95\% level.
Nevertheless coverage results confirm that the confidence intervals constructed using the ML estimates and their standard errors are well-calibrated, with proportions close to the nominal 95\% level. The slight deviations from 95\% are normal and do not raise concerns about the reliability of the ML estimation procedure. The results again underscore the valuable properties of the condtional maximum likelihood estimation that entail, efficiency, consistency and asymptotic normalitly especially for large samples.
\section{Extra}
We could also be verifying stationarity by looking at the characteristic polyonomial in which case the roots z must be bigger then one in absolute terms. We can find z by $\frac{1}{0.95}$ which would yield 1.0526 which is bigger then one, therefore our process would be stationary.



\section{Tablex}
\begin{table}[ht]
    \centering
    \caption{OLS Parameter Estimates vs.\ True Values}
    \label{tab:ols-results}
    \begin{tabular}{lccc}
        \hline
        \textbf{Parameter} & \textbf{Estimate} & \textbf{Std.\ Error} & \textbf{True Value} \\
        \hline
        $\alpha$ & 1.46 & 0.40 & 2.00 \\
        $\rho$   & 0.96 & 0.01 & 0.95 \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[ht!]
\centering
\caption{Dickey-Fuller (DF) Test for Unit Root}
\label{tab:DF-test}

\begin{tabular}{l l}
\toprule
\textbf{Null Hypothesis:}   & $H_0: \rho = 1 \quad (\text{unit root})$ \\
\textbf{Alternative:}       & $H_1: \rho \neq 1 \quad (\text{stationary})$ \\
\midrule
\textbf{Estimate}           & $\hat{\rho} = 0.96 \quad;\quad \mathrm{se}(\hat{\rho}) = 0.01$ \\
\textbf{Test Statistic}     & $t_{\mathrm{DF}} = \frac{0.96 - 1}{0.01} = -3.66$ \\
\textbf{5\% Critical Value} & $-2.86$ \\
\textbf{Decision (5\%)}     & $-3.66 < -2.86 \;\rightarrow\; \text{Reject } H_0$ \\
\bottomrule
\end{tabular}
\end{table}



\begin{table}[h!]
\centering
\begin{tabular}{@{}lcccccc@{}}
\toprule
Case & $c$ & $\phi$ & $\theta$ & $\nu$ & Total Log-Likelihood \\ \midrule
(a)  & 2   & 0.95   & 0.25     & 4     & -1388.32             \\
(b)  & 1.5 & 0.75   & 0.5      & 6     & -5871.19             \\ \bottomrule
\end{tabular}
\caption{Log-likelihood values for the two parameter specifications.}
\label{tab:results} % Label for the table
\end{table}


\begin{table}[h!]
\centering
\caption{Estimated Parameters and Results of the CMLE Procedure for ARMA(1,1)}
\label{tab:results2}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Parameter} & \textbf{True Value} & \textbf{Estimated Value} & \textbf{Standard Error} & \textbf{Covariance Matrix Element} \\ \midrule
$c$      & 2.00     & 2.2456   & 0.404      & 0.1632  \\
$\phi$   & 0.95     & 0.9446   & 0.010      & 0.0001  \\
$\theta$ & 0.25     & 0.2834   & 0.030      & 0.0009  \\
$\nu$    & 4.00     & 3.4515   & 0.347      & 0.1205  \\ \midrule
\textbf{Metric} & \multicolumn{4}{c}{} \\ \midrule
Negative Log-Likelihood & \multicolumn{4}{c}{1386.5} \\
Gradient Magnitude      & \multicolumn{4}{c}{$\|\nabla \hat{L}_{\text{neg}}\| \approx 10^{-5}$} \\
Optimization Exit Flag  & \multicolumn{4}{c}{1 (Successful Convergence)} \\ \bottomrule
\end{tabular}
\end{table}



\section{Parameter Estimates and Confidence Intervals}

The parameter estimates, standard errors, and their 95\% confidence intervals are presented in Table~\ref{tab:ci_results}. These intervals provide insights into the precision of the estimates and allow us to evaluate their statistical significance.

\begin{table}[h!]
\centering
\caption{Parameter Estimates with 95\% Confidence Intervals}
\label{tab:ci_results}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Parameter} & \textbf{Estimate} & \textbf{Standard Error} & \textbf{95\% Confidence Interval} \\ \midrule
$c$      & 2.2312  & 0.4102  & [1.4271, 3.0352] \\
$\phi$   & 0.9452  & 0.0100  & [0.9255, 0.9648] \\
$\theta$ & 0.2837  & 0.0309  & [0.2231, 0.3443] \\
$\nu$    & 3.4691  & 0.3603  & [2.7630, 4.1752] \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Parameter Estimates with \( T = 50,000 \) Observations}
\label{tab:results_large_sample}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Parameter} & \textbf{Estimate} & \textbf{Standard Error (SE)} & \textbf{Covariance Matrix Element} \\ \midrule
$c$      & 1.9604  & 0.0480  & 0.0023  \\
$\phi$   & 0.9511  & 0.0071  & 0.0000  \\
$\theta$ & 0.2425  & 0.0045  & 0.0000  \\
$\nu$    & 3.9719  & 0.0548  & 0.0030  \\ \midrule
\textbf{Metrics} & \multicolumn{3}{c}{} \\ \midrule
Negative Log-Likelihood & \multicolumn{3}{c}{$8.4302 \times 10^4$} \\
Gradient Magnitude      & \multicolumn{3}{c}{$\|\nabla \hat{L}_{\text{neg}}\| \approx 10^{-5}$} \\
Optimization Exit Flag  & \multicolumn{3}{c}{1 (Successful Convergence)} \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Comparison of Parameter Estimates (\( T = 800 \) vs. \( T = 50,000 \))}
\label{tab:param_estimates_comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Parameter} & \textbf{Estimate (\( T = 800 \))} & \textbf{Estimate (\( T = 50,000 \))} \\ \midrule
$c$      & 2.2456  & 1.9604  \\
$\phi$   & 0.9446  & 0.9511  \\
$\theta$ & 0.2834  & 0.2425  \\
$\nu$    & 3.4515  & 3.9719  \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Comparison of Standard Errors (\( T = 800 \) vs. \( T = 50,000 \))}
\label{tab:std_errors_comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Parameter} & \textbf{SE (\( T = 800 \))} & \textbf{SE (\( T = 50,000 \))} \\ \midrule
$c$      & 0.404   & 0.0480  \\
$\phi$   & 0.0100  & 0.0071  \\
$\theta$ & 0.0309  & 0.0045  \\
$\nu$    & 0.347   & 0.0548  \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Comparison of Covariance Matrix Diagonal Elements (\( T = 800 \) vs. \( T = 50,000 \))}
\label{tab:cov_matrix_comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Parameter} & \textbf{Covariance (\( T = 800 \))} & \textbf{Covariance (\( T = 50,000 \))} \\ \midrule
$c$      & 0.1632  & 0.0023  \\
$\phi$   & 0.0001  & 0.0000  \\
$\theta$ & 0.0009  & 0.0000  \\
$\nu$    & 0.1205  & 0.0030  \\ \bottomrule
\end{tabular}
\end{table}




\begin{table}[h!]
\centering
\caption{Quasi-Conditional Maximum Likelihood (QML) Results for ARMA(1,1) (Gaussian Assumption)}
\label{tab:qml_results}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Parameter} & \textbf{True Value} & \textbf{QML Estimate} & \textbf{Standard Error (SE)} \\ \midrule
$c$      & 2.00  & 2.3537  & 0.455 \\ 
$\phi$   & 0.95  & 0.9418  & 0.010 \\ 
$\theta$ & 0.25  & 0.3012  & 0.033 \\ 
$\nu$    & 4.00  & 2.1227  & 0.162 \\ \midrule
\textbf{Metric} & \multicolumn{3}{c}{} \\ \midrule
Negative Log-Likelihood & \multicolumn{3}{c}{1434.4236} \\
Gradient Magnitude      & \multicolumn{3}{c}{$\|\nabla \hat{L}_{\text{neg}}\| \approx 10^{-5}$} \\
Optimization Exit Flag  & \multicolumn{3}{c}{1 (Successful Convergence)} \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{QML Parameter Estimates with 95\% Confidence Intervals (Gaussian Assumption)}
\label{tab:qml_ci_results}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Parameter} & \textbf{Estimate} & \textbf{Standard Error (SE)} & \textbf{95\% CI Lower Bound} & \textbf{95\% CI Upper Bound} \\ \midrule
$c$      & 2.3537  & 0.4550  & 1.4618  & 3.2455  \\
$\phi$   & 0.9418  & 0.0111  & 0.9201  & 0.9635  \\
$\theta$ & 0.3012  & 0.0333  & 0.2359  & 0.3665  \\
$\sigma^2$ & 2.1227 & 0.1624  & 1.8043  & 2.4410  \\ \bottomrule
\end{tabular}
\end{table}




\begin{table}[h!]
\centering
\caption{Coverage Results: Proportion of 95\% Confidence Intervals Containing True Parameter Values}
\label{tab:coverage_results}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Parameter} & \textbf{True Value} & \textbf{Coverage (\%)} \\ \midrule
$c$      & 2.00  & 94.72  \\
$\phi$   & 0.95  & 94.60  \\
$\theta$ & 0.25  & 94.28  \\
$\nu$    & 4.00  & 95.96  \\ \bottomrule
\end{tabular}
\end{table}



\end{document}