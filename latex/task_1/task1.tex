\documentclass[a4paper,12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{hyperref}



\begin{document}
\section{The Data Generating Process}
For the purpose of our analysis we will first introduce our true ARMA(1,1) process which will be assumed to have created the time series data.
This ARMA(1,1) it will entail exactly one autoregressive part and one moving average part. Whereas the autoregressive part will be later of great importance to determine if the process is stationary.
The data generating process will be described in the following way.
\begin{equation}
    y_t = c + \phi y_{t-1} + \theta \epsilon_{t-1} + \epsilon_t
\end{equation}
It is important to mention that the innovations in our process will be t distributed $\epsilon_t \sim t(\nu) $. Moreover, our process will be defined as having the following true parameters:
\begin{equation}
    y_t = 2 + 0.95y_{t-1} + 0.25\epsilon_{t-1} + \epsilon_t
\end{equation}
The innovation terms will follow the t distribution and we will define $\nu$ =4, meaning four degrees of freedom. Hence, the innovation distribution has heavier tails than a Gaussian distribution.
As mentioned in Hamilton to now assess whether our process is stationary we will check whether the autoregressive coefficient ($\phi$) is smaller then one in absolute terms.
For our case we can clearly conclude that $|0.95| <= 1$. 
Therefore, our true process is stationary. We could also assess invertibility but we will neglect this for the time being.
As our process is stationary ($\phi<1$) the coefficients our coefficients are absolutely summable and can define the process in a MA infinity representation which would yield after Hamilton:
\begin{equation}
    \mu + \Phi(L)\epsilon_t
\end{equation}
The mean now can be calculated after Hamilton
\begin{equation}
\frac{c}{1-\phi} = \frac{2}{1-0.95} = 40
\end{equation}
Our expected value is 40 and can be easily calculated as it only depends on the autoregressive parameter, whose value is already known to us.

\section{Realization of the process}
Using the data generating process we have defined in (2) we will now create one realization of the process.
This realization of the process was plotted and can be seen in picture(1).
It is important to mention that for the following graph we simulated 800 observations but due to dependencies in the first 50 observations we decided to discard the first 50 observations as burn in phase.
Moreover, we selected the starting values for our simulation of the realization of the process wich is the same as the expected value of the process, displayed in (4).
A first glance of the graph shows how the data fluctuates around the expected value of 40 which is represented through the dashed red line and subsequently remains close to the theorietical mean which represents a stationary behaviour. 
Furthermore, the data does not display any clear upward or downward moving trend; the process just randomly oscillates around the mean. Additionally, a slight persistance in the data can be seen, as the data entails smoother and less eratic transitions between the successive values. Around the 400 and 500 values region the data eshibts abrupt changes and outliers which can be due to the heavier tails of the t distribution that we defined for the process. Finally the moving average coefficient contributes to short term corrections, which introduce additional randomness to the series but are less dominant compared to the autoregressive component.
In the next sections we will work with the realization of the process asuming that we dont know anything about the data generating process and therefore try to estimate the true process

\section{Dicky Fuller test statistic}
In real world applications researchers mostly face the problem of not knowing about the true data generating process, meaning about which process actaully generated the time series they are looking at and therefore have trouble to say whether certain moments of the series can be extrapolated for the true population moments. 
In this paper we will from now one assuming that we dont know the true process that generated the data and therefore, we will try to conclude from this time series whether the simulated realization is stationary, meaning that the moments of the series are constant over time. Stationarity in this purpose is crucial, as only with stationarity we can also assess whether we encounter erdogicty. 
We want to have erdogicty as in this case a law of large numbers applies and the time series averages will converge either in probability or almost surely towards their expectation counterparts. 
In order to asess whether our data is stationariyt we will use the Dicky Fuller test, which 
To assess stationarity we will now be using the dicky fuller test, which explicitly based on different cases tests whether the process has an unit root or not.
\subsection{Selecting the case}
In order to perform a Dicky Fuller test we will have to decide for a case in order to estimate the model.
It is important to choose as a case which will later yield the alternative hypothesis enough power to reject a false nullhypothesis meaning a rejecting when the process does not have an unit root.
By inspecting the plot we see as mentioned above how the data randomly fluctuates over a nonzero mean which in this case is the value of 40. Therefore we will choose the second case of the dicky fuller test. We will neglect the first case as we there would expect the data to fluctuate around a zero mean which is not the case in our plot and therefore we would provide our stationary alternative with not enough power to reject a false nullhypothesis as the stationary true could seldom produce a fluctuating realization around a nonzero null. Furthermore, we also discared case three and case four. Case 3 will be discarded as we first do not observe a certain druft in the data and moreover, this case yields way to less power to the stationary alternative. Moreover, the fourth case will also be discareded as we mentioned above there is clearly no upward or downward moving trend in the data observable and therefore no reason to decide on the fourth case of the dicky fuller test.
Based on this argumentation the second case of the Dicky Fuller test perfectly aligns with the observations that are depicted in the plot.
\subsection{Estimate the model for the test}
For the second case of the Dicky Fuller test as for Hamilton we would expect the true model under the Nullhypothesis as 
\begin{equation}
    y_t = y_{t-1} + ut
\end{equation}
therefore our parameter $\rho$, which is the parameter of interest would equal the value one. On the other hand the model defining the stationary alternative hypothesis would look the following way
\begin{equation}
    y_t = \alpha + \rho y_{t-1} + ut
\end{equation}
Whereas the $\alpha$ must be bigger then zero and our $\rho <= 1$ in absolute terms. 
Again it can be perfectly seen how this case yields enough power to the alternative hypothesis as the model in the alternative could have created a nonzero mean reverting process as long as the paramters would be defined as above.
For the purpose of conducting such as Dicky Fuller test for the second case we will now estimate our paramters $\hat{\alpha}, \hat{\rho}$ using the model define in equation (6).
\\
After estimating the paramter values using the Ordinary Least Squares approach we obatin $\hat{\rho} = 0.96$ and for $\hat{\alpha}$ = 1.46. The corresponding standard errors are 0.01 for $s.e.(\hat{\rho})$ and 0.4 for $s.e.(\hat{\alpha})$. If we would compare the  paramter estimates with our true parameter values in the second equation, we see that the paramter ($\hat{\rho}$) is very close to the true parameter of $\phi$. On the other hand the value for the constant $\alpha$ differs more noticeably from the true intercept.
This discrepancy is also reflected in the standard errors: the small standard error of $\hat{\rho}$ suggests that the data are very informative about the persistence of the process. By contrast, the higher standard error of $\hat{\alpha}$ arises because, in a near-unit-root setting of $\rho$, even small changes in $\rho$ can can greatly affect the implied mean of the process.Consequently, the intercept estimate is more vulnerable to finite-sample fluctuations and to the influence of the heavy-tailed errors.
Hence, we observe a higher standard error for $\hat{\alpha}$ then for $\hat{\rho}$.

\subsection{Test statistic}
We will now  test for an unit root in $y_t$ using the second case of the Dicky Fuller test. 
We therefore define our Nullhypothesis as 
$$H0 : \rho = 1 \, \text{(unit root)}$$
and the alternative hypothesis as 
$$H1: \rho \neq 1 \, \text{(stationary alternative)}$$
To test whether the process entails an unit root I will define the complete test statistic as the following
$$t-stat: \frac{0.96-1}{0.01}$$
It is important to mention that the Test statistic under the Null is not normally distributed as we are now in the case where we asumme an unit root. Nevertheless, we know that the distribution of the test statistic under the null is nonstandard but as well nuisance parameter free. For our critical values we now cannot use the conventional 1.96 from the normal distribution. Instead we will compare it  against the critical values of the from the Dicky Fuller table.
 For over 500 observations, a zero trend case, and a 5\% significance level, the critical value is approximately -2.86.
The resulting value of our test statistic is -3.66 and therfore is less then -2.86, and therefore we do not fail to reject the nullhypothesis of an unit root for the significance level of 5 percent.
Having established that our process does not contain a unit root, we next estimate its parameters via Maximum Likelihood (ML). 



\section{Conditional Maximum Likelihood estimation}
We now use Conditional Maximum Likelihood in order to estimate the true paramters that created the series. 
Unlike exact Maximum Likelihood, CMLE assumes the first observation, $y_1$, is deterministic and maximizes the likelihood conditioned on this initial value see Hamilton p(122).
This approach is computationally efficient, as noted by Hamilton (p. 123), and under the assumption of a large sample size, the first observation makes a negligible contribution to the total likelihood. 
Given the stationarity condition $\phi < 1$ (proven in the previous section), the Maximum Likelihood Estimators (MLE) and Conditional Maximum Likelihood Estimators (CMLE) share the same large-sample distribution.
For the purpose of conditional maximum likelihood we again as in the prior section assume that the we dont know the true paramters that created the time series.

By maximizing the conditional log-likelihood function with respect to the parameters, we aim to identify the values most likely to have generated the observed data. Additionally, we assume that the process innovations follow a $t$-distribution:
 \begin{equation}
     \epsilon_t \sim t(\nu) 
 \end{equation}
 where $\nu$ denotes the degrees of freedom.
The choice of a $t$-distribution is particularly advantageous due to its ability to model heavy-tailed errors, which are often encountered in real-world data. Unlike Ordinary Least Squares (OLS), which assumes normally distributed errors and minimizes the sum of squared residuals, the correctly defined CMLE accounts for the specific distributional properties of the innovations.
In order two get the maximum likelihood estimates this paper will follow two steps. First we will propose two distinct parameter specifications and compare their respective log-likelihood values. This comparison allows us to determine which specification offers a better fit to the data, thereby providing insights into the underlying dynamics of the process.
Later in the next section we will try to estimate the paramters by numerical search which in this context will be minimizing the log likelihood.
 \subsection{Estimate the Likelihood}
In order to implement our first approach, we compute the conditional log-likelihood contributions for each time step. 
The innovations, $\epsilon_t$, are calculated recursively based on the ARMA(1,1) model:
\begin{equation}
    \epsilon_t = y_t - c -\phi y_{t-1} - \theta \epsilon_{t-1}
\end{equation}
In order to stabilize the maximization of the likelihood by taking the sum of densities instead of the multiplication of densities by taking the logartihm.
The conditional log likelihood contributions are then given by
 \begin{equation}
      l_{t} =  (ln \Gamma(\frac{\nu +1}{2})- ln(\sqrt{\nu \pi} * \Gamma(\frac{\nu}{2})) - \frac{\nu+1}{2}ln(1+\frac{\epsilon_t^{2}}{\nu}))
 \end{equation}
The total conditional log likelihood is then obtained by summing these contributions over all time steps .
\begin{equation}
    L_{total} = \sum_{t=2}^{T} l_t
\end{equation}
We compute the conditional log-likelihood for two parameter specifications:
\begin{itemize}
    \item \textbf{Case (a):} $c = 2, \phi = 0.95, \theta = 0.25, \nu = 4$ (the true parameters used to generate the data),
    \item \textbf{Case (b):} $c = 1.5, \phi = 0.75, \theta = 0.5, \nu = 6$ (an alternative set of parameters).
\end{itemize}
The computed condtional log-likelihood values for each case are summarized in Table~\ref{tab:results}.
As shown in Table~\ref{tab:results}, the conditional log-likelihood value for Case (a) (the true parameter specification) is significantly higher (less negative) than that for Case (b). This indicates that the true parameters provide a better fit to the observed series compared to the alternative specification.
In the context of log-likelihood, "less negative" values are critical because the log-likelihood is derived from the natural logarithm of the joint probability density of the data given the parameters. Higher (or less negative) log-likelihood values correspond to a greater probability density for the observed data, indicating that the specified parameters are more likely to have generated the series.
Therefore, the lower (more negative) log-likelihood value for Case (b) reflects a poorer fit to the data.
The deviations in the AR and MA coefficients, as well as the degree of freedom parameter $\nu$, result in a suboptimal representation of the data-generating process. 
This discrepancy highlights the sensitivity of CMLE to parameter specification. By explicitly accounting for the heavy-tailed $t$-distributed innovations, CMLE effectively distinguishes between parameter sets and favors those that align closely with the true data-generating process.
\subsection{Estimate the paramters with numerical minimization}
In this section we will now try to estimate the parameters $c, \phi, \theta,\nu$ by maximizing the conditional log-likelihood function. To achieve this, we adopt a numerical optimization approach.
It is worth noting that instead of directly maximizing the log-likelihood, we minimize the negative log-likelihood. These two approaches are mathematically equivalent, as minimizing the negative of a function is the same as maximizing the original function. This equivalence stems from the monotonic nature of the negative operation, which preserves the order of the optimization objective.
To adapt our method, we refine the log-likelihood function introduced earlier (Equation 10) to return its negative value. This ensures compatibility with standard numerical optimization algorithms, which are designed to find the minimum of an objective function. In order to minimze the negative of the function we used the 'fminsearch' algorithm from matlab which is also known as derivate free optimization. 
We moreover, specified a Hessian-based covariance matrix to quantify parameter uncertainty and initialized the estimation procedure with the following values c = 1.5, $\phi$ = 0.75, $\theta$ = 0.5, $\nu$ = 5.
The results of the numerical optimiuation can be found in Table~\ref{tab:results2}.
The results of the numerical optimization, presented in Table~\ref{tab:results2}, indicate that the Conditional Maximum Likelihood Estimation (CMLE) approach successfully estimated the parameters of the ARMA(1,1) process. The estimated values for \(c\), \(\phi\), \(\theta\), and \(\nu\) are close to their true values, demonstrating the robustness and accuracy of the method.

Specifically:
\begin{itemize}
    \item The estimated constant term, \(\hat{c} = 2.2456\), closely approximates the true value of \(c = 2\).
    \item The autoregressive coefficient, \(\hat{\phi} = 0.9446\), is near the true value of \(\phi = 0.95\), indicating accurate capture of the process's dependency structure.
    \item The moving average coefficient, \(\hat{\theta} = 0.2834\), is similarly aligned with the true value of \(\theta = 0.25\).
    \item The degrees of freedom for the \(t\)-distributed innovations, \(\hat{\nu} = 3.4515\), suggests that the estimation effectively captures the heavy-tailed nature of the errors, with only minor deviations from the true \(\nu = 4\).
\end{itemize}

The standard errors derived from the Hessian-based covariance matrix indicate reasonable precision for all parameter estimates, with relatively small variances. The minimized negative log-likelihood value of 1386.5 confirms the plausibility of the parameter estimates, while the small gradient magnitude at the optimum (\(\|\nabla \hat{L}_{\text{neg}}\| \approx 10^{-5}\)) and a successful optimization exit flag further validate the convergence of the procedure. It is worth noting that the minimized negative log-likelihood value of 1386.5 is slightly better than the positive log-likelihood value of the true parameter process, which was \(-1388.32\). This small improvement could arise due to the flexibility of the numerical optimization in finding parameter estimates that fit the observed realization of the series slightly better than the true population parameters. This phenomenon is possible because the observed data is a finite realization of the process, and the optimization procedure tailors the estimates specifically to this realization, potentially compensating for random variations in the sample.
In conclusion, the CMLE approach, coupled with numerical optimization, provides a reliable and efficient framework for parameter estimation in ARMA(1,1) processes. The results demonstrate the practical utility of this methodology in accurately recovering the underlying parameters of a data-generating process.

\subsection{Confidence intervalls}
After estimating the paramters of the ARMA(1,1) process using numerical optimization we now want to assess the precission of the estimates by calculating the standard errors and constructing 95\% confidence intervals.
Confidence intervals are essential for determining the range within which the true parameter values are likely to lie, given the data and model assumptions.The 95\% confidence intervals are constructed based on the standard errors derived from the Hessian-based covariance matrix. These intervals provide a statistical test for the null hypothesis that the true parameter values are equal to specific estimates. 
For values within the confidence interval, we fail to reject the null hypothesis at the 5\% significance level, suggesting that these values are consistent with the observed data. This will help us to evaluate the reliability and uncertainty of the paramter estimates.
Due to the asymptotic normality assumption of the Maximum Likelihood estimates we will treat the paramters as normally distributes even though our innovations are t distributed, this is due to our large sample size. 
The results for our confidence intervalls which are displayed in ~\ref{tab:ci_results} and indicate that first the estimate of \(c = 2.2312\) falls within the confidence interval \([1.4271, 3.0352]\), underscoring that we could not reject the nullhypothesis that our estimate is the true value at least for 5 percent signifcance. On the other hand the interval is relatively wide compared to the other parameters, indicating higher uncertainty in the estimate. The estimate of \(\phi = 0.9452\) is highly precise, with a narrow confidence interval \([0.9255, 0.9648]\). The confidnece intervall moreover, excludes one and therefore confirms the stationarity of the process. The estimate of \(\theta = 0.2837\) lies within the interval \([0.2231, 0.3443]\), demonstrating moderate precision, still we could not reject the nullhypothesis indicating that this value is the true paramter value.  The estimate of \(\nu = 3.4691\) suggests that the error distribution has heavy tails, as the interval \([2.7630, 4.1752]\) excludes values consistent with normality (\(\nu \to \infty\)).
Overall, the results confirm the robustness of the parameter estimates, with the confidence intervals supporting their statistical significance and alignment with the underlying process characteristics. 


\section{Hypothesis testing}
With the parameter estimates and the confidence intervalls established, we now focus on hypothesis testing to assess specific claims about the paramters of our ARMA(1,1) model. 
We therefore will consider the null hypothesis $H0: \phi =0.8$, testing whether the autoregressive coefficient is significantly different from this hypothesized value.  Using the t-test and a significance level of 5\%, we aim to evaluate whether the data provides sufficient evidence to reject this hypothesis.
To test whether the autoregressive coefficient \(\phi\) is significantly different from 0.8, we perform a t-test using the following hypotheses:

\[
H_0: \phi = 0.8 \quad \text{(The null hypothesis)}
\]
\[
H_1: \phi \neq 0.8 \quad \text{(The alternative hypothesis)}
\]

The test statistic for the t-test is calculated as:
\[
t = \frac{\hat{\phi} - \phi_0}{\text{SE}(\hat{\phi})},
\]
where:
\begin{itemize}
    \item \( \hat{\phi} \) is the estimated value of the parameter,
    \item \( \phi_0 = 0.8 \) is the hypothesized value under \( H_0 \),
    \item \( \text{SE}(\hat{\phi}) \) is the standard error of the estimated parameter.
\end{itemize}

Given that the standard error of \( \hat{\phi} \) is \( \text{SE}(\hat{\phi}) = 0.0100 \), and the estimated value is \( \hat{\phi} = 0.9452 \), the t-statistic becomes:
\[
t = \frac{0.9452 - 0.8}{0.0100} = \frac{0.1452}{0.0100} = 14.52.
\]
For a two-tailed t-test at the 5\% significance level (\(\alpha = 0.05\)), the critical value is:
\[
t_{\text{crit}} = \pm 1.96,
\]
assuming the sample size is large enough for the test statistic to follow a standard normal distribution.
The calculated t-statistic is \( t = 14.52 \), which is much greater than the critical value of \( t_{\text{crit}} = 1.96 \). Therefore, we do not fail to reject the null hypothesis \( H_0: \phi = 0.8 \) at the 5\% significance level.
This again underlines a stronger persistence in the time series than would be expected under the null hypothesis.


\subsection{Two-Sided p-Value}
We moreover, calculated the two-sided p-value, which quantifies the probability of observing a test statistic as extreme as \( t = 14.4996 \), assuming \( H_0 \) is true. The p-value is computed as:
\[
p = 2 \cdot (1 - \Phi(|t|)),
\]
where \( \Phi(|t|) \) is the cumulative distribution function (CDF) of the standard normal distribution. Substituting \( t = 14.4996 \), we find:
\[
p = 2 \cdot (1 - \Phi(14.4996)) \approx 0.000000.
\]
Since \( p < 0.05 \), we do not fail to reject the null hypothesis \( H_0 \) at the 5\% significance level. This result is consistent with the conclusion of the t-test, further reinforcing the finding that the estimated autoregressive coefficient \(\phi\) is significantly different from 0.8.
The extremely low p-value (\( p \approx 0.000000 \)) indicates that the likelihood of observing such a large test statistic under the null hypothesis is essentially zero. This provides strong evidence against \( H_0 \), suggesting that the persistence in the time series, as captured by \(\phi\), is significantly greater than the hypothesized value of 0.8.

\subsection{Sample size}
To assess the impact of sample size on paramter estimation and hypothesis testing, we increase the number of observations for the ARMA(1,1) process from T=800 to T=50000. By repeating the estimation of the paramters with conditional maximum likelihood, computing the confidence intervalls and finally test the hypothesis, we evaluate how a significantly larger sample influences the precision of parameter estimates, the results of hypothesis tests, and the computation of the covariance matrix.
The parameter estimates obtained with T=50,000 observations are presented in Table~\ref{tab:results_large_sample}.
For this task we used the same starting values as for the first approach, and moreover we again minimized the negative of condtional log likelihood using a computational approach. 
Table~\ref{tab:param_estimates_comparison} shows the comparison of the paramter estimates for different number of observations. We can clearly see how increasing the sample size improves both accuracy and precision of the paramter estimates, as evidenced by estimates closer to the true values for all four paramters. 
Furtheremore, across all parameters, standard errors decrease dramatically with the increase in sample size, reflecting improved precision. For instance: For c the standard error decreases from 0.40 to 0.05. 
We moreover can observe how the negative log likelihood increases from 386.5 (T=800) to 84,302 (T=50,000). This increase is expected due to the larger sample size, as the log-likelihood scales with the number of observations.
Finally Table~\ref{tab:cov_matrix_comparison} shows how the covarinace matrix shrinks consistently, indicating a reduced uncertainty in the paramter estimates as T increases. 
The increase in the number of observations from 800 to 50000 highlights the importance of the sample size, as it not only reduces the estimations the estimation errors but also provides reliable measures of uncertainty.








\section{Extra}
We could also be verifying stationarity by looking at the characteristic polyonomial in which case the roots z must be bigger then one in absolute terms. We can find z by $\frac{1}{0.95}$ which would yield 1.0526 which is bigger then one, therefore our process would be stationary.



\section{Tablex}
\begin{table}[ht]
    \centering
    \caption{OLS Parameter Estimates vs.\ True Values}
    \label{tab:ols-results}
    \begin{tabular}{lccc}
        \hline
        \textbf{Parameter} & \textbf{Estimate} & \textbf{Std.\ Error} & \textbf{True Value} \\
        \hline
        $\alpha$ & 1.46 & 0.40 & 2.00 \\
        $\rho$   & 0.96 & 0.01 & 0.95 \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[ht!]
\centering
\caption{Dickey-Fuller (DF) Test for Unit Root}
\label{tab:DF-test}

\begin{tabular}{l l}
\toprule
\textbf{Null Hypothesis:}   & $H_0: \rho = 1 \quad (\text{unit root})$ \\
\textbf{Alternative:}       & $H_1: \rho \neq 1 \quad (\text{stationary})$ \\
\midrule
\textbf{Estimate}           & $\hat{\rho} = 0.96 \quad;\quad \mathrm{se}(\hat{\rho}) = 0.01$ \\
\textbf{Test Statistic}     & $t_{\mathrm{DF}} = \frac{0.96 - 1}{0.01} = -3.66$ \\
\textbf{5\% Critical Value} & $-2.86$ \\
\textbf{Decision (5\%)}     & $-3.66 < -2.86 \;\rightarrow\; \text{Reject } H_0$ \\
\bottomrule
\end{tabular}
\end{table}



\begin{table}[h!]
\centering
\begin{tabular}{@{}lcccccc@{}}
\toprule
Case & $c$ & $\phi$ & $\theta$ & $\nu$ & Total Log-Likelihood \\ \midrule
(a)  & 2   & 0.95   & 0.25     & 4     & -1388.32             \\
(b)  & 1.5 & 0.75   & 0.5      & 6     & -5871.19             \\ \bottomrule
\end{tabular}
\caption{Log-likelihood values for the two parameter specifications.}
\label{tab:results} % Label for the table
\end{table}


\begin{table}[h!]
\centering
\caption{Estimated Parameters and Results of the CMLE Procedure for ARMA(1,1)}
\label{tab:results2}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Parameter} & \textbf{True Value} & \textbf{Estimated Value} & \textbf{Standard Error} & \textbf{Covariance Matrix Element} \\ \midrule
$c$      & 2.00     & 2.2456   & 0.404      & 0.1632  \\
$\phi$   & 0.95     & 0.9446   & 0.010      & 0.0001  \\
$\theta$ & 0.25     & 0.2834   & 0.030      & 0.0009  \\
$\nu$    & 4.00     & 3.4515   & 0.347      & 0.1205  \\ \midrule
\textbf{Metric} & \multicolumn{4}{c}{} \\ \midrule
Negative Log-Likelihood & \multicolumn{4}{c}{1386.5} \\
Gradient Magnitude      & \multicolumn{4}{c}{$\|\nabla \hat{L}_{\text{neg}}\| \approx 10^{-5}$} \\
Optimization Exit Flag  & \multicolumn{4}{c}{1 (Successful Convergence)} \\ \bottomrule
\end{tabular}
\end{table}



\section{Parameter Estimates and Confidence Intervals}

The parameter estimates, standard errors, and their 95\% confidence intervals are presented in Table~\ref{tab:ci_results}. These intervals provide insights into the precision of the estimates and allow us to evaluate their statistical significance.

\begin{table}[h!]
\centering
\caption{Parameter Estimates with 95\% Confidence Intervals}
\label{tab:ci_results}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Parameter} & \textbf{Estimate} & \textbf{Standard Error} & \textbf{95\% Confidence Interval} \\ \midrule
$c$      & 2.2312  & 0.4102  & [1.4271, 3.0352] \\
$\phi$   & 0.9452  & 0.0100  & [0.9255, 0.9648] \\
$\theta$ & 0.2837  & 0.0309  & [0.2231, 0.3443] \\
$\nu$    & 3.4691  & 0.3603  & [2.7630, 4.1752] \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Parameter Estimates with \( T = 50,000 \) Observations}
\label{tab:results_large_sample}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Parameter} & \textbf{Estimate} & \textbf{Standard Error (SE)} & \textbf{Covariance Matrix Element} \\ \midrule
$c$      & 1.9604  & 0.0480  & 0.0023  \\
$\phi$   & 0.9511  & 0.0071  & 0.0000  \\
$\theta$ & 0.2425  & 0.0045  & 0.0000  \\
$\nu$    & 3.9719  & 0.0548  & 0.0030  \\ \midrule
\textbf{Metrics} & \multicolumn{3}{c}{} \\ \midrule
Negative Log-Likelihood & \multicolumn{3}{c}{$8.4302 \times 10^4$} \\
Gradient Magnitude      & \multicolumn{3}{c}{$\|\nabla \hat{L}_{\text{neg}}\| \approx 10^{-5}$} \\
Optimization Exit Flag  & \multicolumn{3}{c}{1 (Successful Convergence)} \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Comparison of Parameter Estimates (\( T = 800 \) vs. \( T = 50,000 \))}
\label{tab:param_estimates_comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Parameter} & \textbf{Estimate (\( T = 800 \))} & \textbf{Estimate (\( T = 50,000 \))} \\ \midrule
$c$      & 2.2456  & 1.9604  \\
$\phi$   & 0.9446  & 0.9511  \\
$\theta$ & 0.2834  & 0.2425  \\
$\nu$    & 3.4515  & 3.9719  \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Comparison of Standard Errors (\( T = 800 \) vs. \( T = 50,000 \))}
\label{tab:std_errors_comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Parameter} & \textbf{SE (\( T = 800 \))} & \textbf{SE (\( T = 50,000 \))} \\ \midrule
$c$      & 0.404   & 0.0480  \\
$\phi$   & 0.0100  & 0.0071  \\
$\theta$ & 0.0309  & 0.0045  \\
$\nu$    & 0.347   & 0.0548  \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Comparison of Covariance Matrix Diagonal Elements (\( T = 800 \) vs. \( T = 50,000 \))}
\label{tab:cov_matrix_comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Parameter} & \textbf{Covariance (\( T = 800 \))} & \textbf{Covariance (\( T = 50,000 \))} \\ \midrule
$c$      & 0.1632  & 0.0023  \\
$\phi$   & 0.0001  & 0.0000  \\
$\theta$ & 0.0009  & 0.0000  \\
$\nu$    & 0.1205  & 0.0030  \\ \bottomrule
\end{tabular}
\end{table}

\end{document}