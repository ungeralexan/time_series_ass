\documentclass[12pt]{article}
\usepackage[a4paper, left=2.5cm, top=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{float}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{caption}

\begin{document}
\clearpage
\title{ATS Assignment}
\author{601223}
\date{Date of Submission: January 12}

\begin{titlepage}
\textbf{\Large ATS Assignment}
\end{titlepage}

\tableofcontents

\newpage

\setstretch{1.2}
\section{Introduction}
Time series data are prevalent not only in traditional fields such as economics and finance but also in emerging areas like engineering and machine learning. 
Analyzing and understanding the underlying process that generates time series data is critical for tasks such as forecasting or hypothesis testing.
A key step in this analysis is the identification and application of appropriate models, with Autoregressive Moving Average (ARMA) models being particularly popular due to their effective combination of autoregressive and moving average components.
However, a fundamental challenge arises from the fact that, in practice, we often observe only a single realization of the stochastic process. This limitation necessitates reliance on statistical properties like stationarity and ergodicity, which allow us to make meaningful inferences about the true data-generating process from the observed series.
Once stationarity is established, the next crucial task is the estimation of parameters. Here Maximum Likelihood provides a rigorous framework for this purpose, offering desirable properties such as consistency and efficiency when the likelihood function is correctly specified. However, specifying the correct likelihood often requires substantial knowledge about the underlying process, which may not always be available. In such cases, quasi-maximum likelihood estimation (QMLE) serves as an alternative, allowing for approximate likelihood functions that can still yield valuable parameter estimates under broader conditions.
In our paper, we aim to bridge the gap between theory and application.
We will simulate a realization of a process whose true parameter specification follows an ARMA(1,1) model. Given the presence of stationarity, which we will later test using the Dickey-Fuller test, we will simulate two scenarios: one where the true likelihood function is defined (assuming we know the true distribution of the innovations) and one with a misspecified likelihood.
We will begin by estimating the parameters through numerical optimization, minimizing the negative log-likelihood. For the scenario with the true likelihood, we will explore the impact of increasing the sample size on the precision of our estimates. Additionally, we will repeat the simulations for a fixed sample size to evaluate the confidence intervals of the estimated parameters.
Finally, we will analyze the case of likelihood misspecification, examining how the parameter estimates are affected and identifying the properties of these estimates under misspecification.
\section{The true data generating Process}
For the purpose of our analysis we will first introduce our true ARMA model which is an ARMA(1,1) model with the parameter specification that can be seen in Equation\eqref{eq:arma}.
The innovation terms of our process $\epsilon_t$ will be t distributed, where we will define $\nu$ =4, meaning four degrees of freedom. Hence, the distribution of the innovations has heavier tails tails than the Gaussian distribution.\\
As mentioned in Hamilton in order to assess whether the process is stationary or not one has to check the autoregressive especifically whether the autoregressive coefficient ($\phi$) is smaller then one in absolute terms. 
As the autoregressive coefficient is smaller then one in absolute terms what can be seen, we can conclude that the true data generating process from whom we simulate the data is stationary.
As our process is stationary, the coefficients our coefficients are absolutely summable and can define the process in a MA infinity representation which would yield after Hamilton Equation\eqref{eq:Mainf}.
For simulation purpose we created a MATLAB function that simulates the data given the parameter values we defined in \eqref{eq:arma}.
To create our data we simulated 800 observations, but due to dependencies in the first 50 observations we decided to discard the first 50 observations as burn in phase. Moreover, we set the starting values for our simulation equal to the expected value of the process which can be seen in Equation\eqref{eq:Mean}.

\section{Dicky Fuller Test}
From this point onward, we shift from the theoretical framework to a practical scenario where the underlying process generating the time series data is unknown. 
This uncertainty poses challenges when attempting to extrapolate moments of the series to represent the true population moments.
To address this and in order to later enable a meaningful parameter estimatuon with Maximum Likelihood, we first need to confirm the stationariyt of the process.
Therefore, in this section we will conduct the Dickey-Fuller test, which evaluates whether the series contains a uni root.
A possible presence of a unit root would indicate non-stationarity and therefore, make it the realization unsuitable for the assumptions required in our subsequent analysis.
To perform the Dickey-Fuller test, we first need to choose an appropriate case for our estimated model.
This descision will be guided by a visual inspection of Figure~\ref{fig:armaone}., which will determine the most suitable test specification.
\subsection{The case of the Dickey Fuller test}
Figure~\ref{fig:armaone} clearly demonstrates that the series fluctuates randomly around a nonzero mean, which in this case is 40. This mean is highlighted by the red line in the plot.
Given this observation we aim to choose a test specification that yields enough power to the stationary alternative i.e., its ability to reject a false null hypothesis.
The second case of the Dickey-Fuller test is most appropriate here because the alternative model in this case can account for a process with random fluctuations around a nonzero mean. \\
If we were to choose the first case of the Dickey-Fuller test, it would assume the data fluctuates around a zero mean, which does not align with the observed behavior of our data.
 This mismatch would result in reduced power for the stationary alternative, making it less likely to detect stationarity when it exists.\\
Similarly, the third and fourth cases can also be discarded. Case 3 is unsuitable because we do not observe a drift in the data, and Case 4 is not applicable as there is no discernible upward or downward trend. Selecting either of these cases would again fail to provide the stationary alternative with sufficient power to capture the true behavior of the process.\\
Thus, based on these considerations, the second case of the Dickey-Fuller test aligns perfectly with the characteristics of the data as shown in the plot.\\
For the second case of the Dickey-Fuller test, we will first consider the model specified under the null hypothesis, as shown in \eqref{eq:true}, and the corresponding alternative hypothesis model, depicted in \eqref{eq:alternative}. Based on the data, we will estimate the alternative model outlined in \eqref{eq:alternative}, which is appropriate for this test case.
Using Ordinary Least Squares (OLS), we estimate the parameters $\hat{\alpha}, \hat{\rho}$ obtaining $\hat{\rho} = 0.96$ and $\hat{\alpha}$ = 1.46. The corresponding standard errors are $s.e.(\hat{\rho})$=0.01 and 
$s.e.(\hat{\alpha})$ = 0.4, providing insight into the precision of these estimates.
If we would compare the  paramter estimates with our true parameter values in the second equation, we see that the paramter ($\hat{\rho}$) is very close to the true parameter of $\phi$. On the other hand the value for the constant $\alpha$ differs more noticeably from the true intercept. This is moreover underscored by the standard errors.
\subsubsection{Test statistic}
We will now  test for an unit root in $y_t$ using the second case of the Dicky Fuller test. 
We therefore define our Nullhypothesis and alternative hypothesis in Equation\eqref{eq:dftest}.
To test whether the series entails an unit root or not the complete test statistic is defined in Equation\eqref{eq:dftstat2}.
It is important to mention that the Test statistic under the Null is not normally distributed as we are now in the case where we asumme an unit root. Nevertheless, we know that the distribution of the test statistic under the null is nonstandard but as well nuisance parameter free. For our critical values we now cannot use the conventional 1.96 from the normal distribution. Instead we will compare it  against the critical values of the from the Dicky Fuller table.
For over 500 observations, a zero trend case, and a 5\% significance level, the critical value is approximately -2.86.
The resulting value of our test statistic is -3.66 and therfore is less then -2.86, and therefore we do not fail to reject the nullhypothesis of an unit root for the significance level of 5 percent.\\
 Having confirmed that the process does not contain a unit root, we will proceed to estimate its parameters using Maximum Likelihood (ML). Initially, we will assume that the correct density is specified, and subsequently, we will explore the effects of misspecifying the density.

\section{Maximum Likelihood estimation}
For the parameter estimation, we will now employ Maximum Likelihood, specifically Conditional Maximum Likelihood (CMLE). 
Unlike exact Maximum Likelihood, CMLE assumes the first observation, $y_1$, is deterministic and maximizes the likelihood conditioned on this initial value see Hamilton p(122).
This approach is computationally efficient, as noted by Hamilton (p. 123), and under the assumption of a large sample size, the first observation makes a negligible contribution to the total likelihood. 
Given the stationarity condition $\phi < 1$ (proven in the previous section), the Maximum Likelihood Estimators (MLE) and Conditional Maximum Likelihood Estimators (CMLE) share the same large-sample distribution.\\
In order to get the maximum likelihood estimates this paper will follow two steps. First we will propose two distinct parameter specifications and compare their respective log-likelihood values. This comparison allows us to determine which specification offers a better fit to the data, thereby providing insights into the underlying dynamics of the process.
Afterwards we will try to estimate the paramters by numerical search which in this context will be minimizing the log likelihood.
 \subsection{Estimate the Likelihood}
In order to implement our first approach, we compute the conditional log-likelihood contributions for each time step. Therefore we have to construct the likelihood function specifyng the underlying densities for each observation. 
Assuming we are in a situation where we correctly specify our densities knowing that the innovations of the process follow a t distribution our conditional log likelihood contributions can be seen in Equation \eqref{eq:clc}.
To stabilize the maximization process, we work with the log of the densities rather than the densities themselves. This transformation allows us to sum the log-densities instead of multiplying the densities, improving numerical stability.
The total conditional log likelihood is then obtained by summing these contributions over all time steps, as it can be seen in Equation \eqref{eq:tcl}.
In the following analysis, we will use two parameter specifications to compute the conditional likelihood. First, we will calculate the conditional log-likelihood for the true parameters that generated the series. We will then compare this value to the log-likelihood obtained using an alternative parameter specification. The two parameter specifications are presented in \ref{specifica}. The results are displayed in Table~\ref{tab:log_likelihood_results}.
It can clearly be seen, that the conditional log-likelihood value for Case (a) (the true parameter specification) is notably higher (less negative) than that for Case (b). This indicates that the true parameters provide a better fit to the observed series compared to the alternative specification.
In the context of log-likelihood, higher (or less negative) values indicate greater probability density for the observed data, suggesting that the specified parameters are more likely to have generated the series.
The deviations in the AR and MA coefficients, as well as the degree of freedom parameter $\nu$, result in a suboptimal representation of the data-generating process. 
This discrepancy underscores the sensitivity of CMLE when it comes to parameter specification.
By explicitly accounting for the heavy-tailed $t$-distributed innovations, CMLE effectively distinguishes between parameter sets and favors those that align closely with the true data-generating process.

\subsection{Estimate the parameters using numerical minimization}
In order to estimate the parameters $c, \phi, \theta,\nu$ we will be minimizing the negative log-likelihood. We therefore adopt a numerical optimization approach. It is worth noting that these two approaches are mathematically equivalent, as minimizing the negative of a function is the same as maximizing the original function. This equivalence stems from the monotonic nature of the negative operation, which preserves the order of the optimization objective.
To adapt our method, we refine the log-likelihood function introduced earlier Equation \eqref{eq:tcl} to return its negative value.
We convert that in order to ensure compatibility with standard numerical optimization algorithms, which are designed to find the minimum of an objective function.
In order to minimze the negative of the function we used the 'fminsearch' algorithm from MATLAB which is also known as derivate free optimization. 
We moreover, specified a Hessian-based covariance matrix to quantify parameter uncertainty and initialized the estimation procedure with the following values c = 1.5, $\phi$ = 0.75, $\theta$ = 0.5, $\nu$ = 5.
The results of the numerical optimization can be found in Table~\ref{tab:optimization_results}. Our results indicate that the Conditional Maximum Likelihood Estimation (CMLE) approach successfully estimated the parameters of the ARMA(1,1) process.
The estimated values for \(c\), \(\phi\), \(\theta\), and \(\nu\) are close to their true values, demonstrating the accuracy of the method.
The constant term  \(\hat{c} = 2.2456\), closely approximates the true value of \(c = 2\),  while the autoregressive coefficient  \(\hat{\phi} = 0.9446\) is near the true \(\phi = 0.95\), effectively capturing the process's dependency structure. Similarly, the moving average coefficient \(\hat{\theta} = 0.2834\) aligns well with the true \(\theta = 0.25\). Finally, the estimated degrees of freedom for the \(t\)-distributed innovations \(\hat{\nu} = 3.4515\) closely reflect the true \(\nu = 4\), indicating the model's ability to capture the heavy-tailed nature of the errors.
The standard errors derived from the Hessian-based covariance matrix indicate reasonable precision for all parameter estimates, with relatively small variances. Furtheremore, the minimized negative log-likelihood value of 1386.5 confirms the plausibility of the parameter estimates, while the small gradient magnitude at the optimum (\(\|\nabla \hat{L}_{\text{neg}}\| \approx 10^{-5}\)). It is worth noting that the minimized negative log-likelihood value of 1386.5 is slightly better than the positive log-likelihood value of the true parameter process, which was \(-1388.32\). This small improvement could arise due to the flexibility of the numerical optimization in finding parameter estimates that fit the observed realization of the series slightly better than the true population parameters. This phenomenon is possible because the observed data is a finite realization of the process, and the optimization procedure tailors the estimates specifically to this realization, potentially compensating for random variations in the sample.

\subsection{Confidence intervalls}
To further assess the precision of our computed estimates we constructed the 95\% confidence intervals.
The intervals are essential providing the range within which the true parameter values are likeli to lie, given the data.
For values within the confidence interval, we fail to reject the null hypothesis at the 5\% significance level, that these values equal the true values of the parameters.
As the Maximum Likelihood estimation of the parameter yields asymptotic normality we will use the conventional quantiles of the normal distribution when computing the 95\% confidence intervals.
The results for our confidence intervals are displayed in Table~\ref{tab:firstconfidence_intervals}.
The results indicate that all our estimates fall within the resepective 95\% confidence intervalls highlighting that we could not reject the nullhypothesis that our estimates equal the true value at least for a 5 percent significance level.
Nevertheless, we want to emphasize on the fact the the intervals especially for our parameters $c \, \text{and} \, \nu$ are relatively wide compared to those of the other parameters, indicating higher uncertainty in the estimate.
Overall, the results underline the statistical significance of our estimates and therefore also highlight one of the properties of Maximum Likelihood which is the consistency of our estimates.
\subsection{T-test}
Building on the insights gained from the confidence intervals, we now turn to hypothesis testing.
Specifically, we aim to test whether the autoregressive coefficient $\phi$ differs significantly from the hypothesized value of 0.8.
Using the t-test and a significance level of 5\%, we aim to evaluate whether the data provides sufficient evidence to reject this hypothesis.
The Nullhypothesis that claims that the true value equals the value of 0.8 and the Alternative hypothesis, which states that the true values is not equal the value of 0.8 can be viewed in Equation \ref{eq:nullacht}. Moreover, we define the test statistic in Equation \ref{eq:tstatmlone}.
For our critical values we will use the conventional 1.96 from the normal distribution, as we assume that under a large sample size the maximum likelihood estimate is asymptotically normal distributed and therefore the test statistic under the nullhypothesis as well follows a normal distribution.
The calculated t-statistic is \( t = 14.52 \), which is much greater than the critical value of \( t_{\text{crit}} = 1.96 \). Therefore, we  reject the null hypothesis \( H_0: \phi = 0.8 \) at the 5\% significance level.
The result is underscored by the two sided p-value we calculated, which quantifies the probability of observing a test statistic as extreme as \( t = 14.4996 \), assuming \( H_0 \) is true.
Since \( p < 0.05 \), we can reject the null hypothesis that the true value equals 0.8 at a 5 \% significance level. 
\subsection{Increase sample size}
To explore the key properties of Maximum Likelihood Estimators—efficiency, consistency, and asymptotic normality—we will first increase the sample size of the realization of the process. Subsequently, in a less conventional approach, we will keep the sample size constant but perform repeated simulations of the process. These simulations aim to provide deeper insights into the behavior of Maximum Likelihood Estimation, particularly the distribution of the parameter estimates across different realizations of the process.
To assess the impact of an increased sample size, we will expand the sample size from 800 to 50,000 observations and repeat the parameter estimation using Maximum Likelihood. By re-estimating the parameters with Conditional Maximum Likelihood, constructing confidence intervals, and performing hypothesis tests, we aim to evaluate how a substantially larger sample size affects the precision of parameter estimates, the outcomes of hypothesis tests, and the computation of the covariance matrix.
The parameter estimates obtained with T=50,000 observations are presented in Table~\ref{tab:optimization_large_sample}. Again for the estimation with the numerical optimization minimizing the negative likelihood we used the same starting values as for the first approach.
The estimates clearly show how increasing the sample size improves both accuracy and precision of the paramter estimates, as evidenced by estimates closer to the true values for all four parameters.
Furtheremore, Table~\ref{tab:confidence_intervals_large_sample} shows that across all parameters the standard erros decrease dramatically with the increase in the sample size, reflecting the improved precision.
For instance: For c the standard error decreases from 0.40 to 0.05. 
The covariance matrix as well shrinks consitently, indicating a reduced uncertainty in the parameter estimates as the sample size increases
We moreover can observe how the negative log likelihood increases from 1386.5 (T=800) to 84,302 (T=50,000). 
This increase is expected due to the larger sample size, as the log-likelihood scales with the number of observations.
Additionally, it is visible how 95\% confidence intervalls shrank in comparison to the estimated parameters when the smaller sample size.
Regarding the significance of our parameter estimates the results indicate that all our estimates fall within the resepective confidence intervalls underscoring that we could could not reject the nullhypothesis that our estimates equal the true value at least for 5 percent significance level.
If we would again define the Nullhypothesis which is depicted in Equation  \ref{eq:nullacht}, whether the true values euqals the value of 0.8 or not, we will now have to change the test statistic to the test statistic shown in Equation \ref{eq:tstatmltwo}.
As for the asymptotic normality property for our parameter estimate that is yielded by the Maximum likelihood estimation we again asume that our test statistic is asymptotically normal distributed. We therefore again use the critical value of 1.96 for a significance level of 5\.%
As the value of our test statistic exceeds the critical value of 1.96 clearly we can reject the Nullhypothesis that the true value equals 0.8 for a significance level of 5\%.
The increase of the sample size once again underlines the importnat properties that the maximum likelihood estimation technique yields when we correctly specify the likelihood function.These properties entail consistency, efficiency and asymptotic normality.
\subsection{Simulate various realizations of the process.}
An alternative approach to estimating the parameters of a stochastic process involves simulating multiple realizations of the process, each of the same length. 
While this method is typically not feasible in real-world applications—where only a single realization of the process is observed—it provides valuable insights in a theoretical scenario. Specifically, by assuming we know the true parameter values of the process, we can simulate independent realizations to explore how Maximum Likelihood Estimates (MLEs) vary around the true parameters.
To approximate the variability of the MLE for a given sample size, we simulate 2,500 realizations of the process. For each realization, we estimate the parameter values using MLE. This exercise allows us to study the distribution of the parameter estimates and with that gain a deeper understanding of the estimator's finite-sample properties.
To analyze the results of our 2,500 simulated estimates, we present visualizations using kernel density plots for the parameter estimates  $c, \phi,\theta$.
Kernel density plots offer a smooth and continuous representation of the distribution of these estimates, helping to illustrate their variability and central tendencies.
The first plot which displays the estimation of the intercept term $c$ estimates across 2500 realizations.The density is clearly centered around the true value of 2, with a slight spread indicating some variability. Nevertheless, the distribution is tightly clustered, confirming that the estimates consistently hover near the true value.
The second plot depicts the distribution of estimates for $\phi$. This distribution is sharply peaked around the true value of 0.95 with minimal variance, highlighting the high precision of the MLE for this parameter. The narrow spread underscores that the vast majority of estimates lie very close to 0.95.
Finally for the $\theta$ estimate we again see that the density is tightly centered near the true $\theta$ value of 0.25. It indicates that the MLE effectively recovers the true parameter values for $\theta$ as well.
To further assess the performance of the estimators, we constructed 95\% confidence intervals for each parameter and evaluated their coverage probabilities—i.e., the proportion of intervals that capture the true parameter value.The results, shown in Table 11, reveal that the coverage for 
c (94.72\%) and $\phi$ (95.01\%) are very close to the nominal 95\% level, aligning with the patterns observed in the kernel density plots. 
For $\theta$ the coverage is slightly lower at 94.28\%, and for the $\nu$ parameter the coverage slightly exceeds the nominal 95\% level.
The coverage results confirm that the confidence intervls cocnstructed using the ML estimates and their standard errors are well calibrated with proportions close to the nominal 95\% level.
The results clearly demonstrate that MLE yields parameter estimates that are close to the true values across different realizations of the process. This provides valuable insights into the reliability and variability of MLE in finite-sample settings.












\newpage
\section{Equations}
\textbf{The equation for our true generated ARMA model}
\begin{equation}
    y_t = 2 + 0.95y_{t-1} + 0.25\epsilon_{t-1} + \epsilon_t
\label{eq:arma}
\end{equation}
Where the innovations $\epsilon_t$ are t distributed.
 \begin{equation}
     \epsilon_t \sim t(\nu) 
 \end{equation}
\textbf{Stationarity test}
\begin{equation}
    |0.95| <= 1
    \label{eq:stat}
\end{equation}

\textbf{Ma infinity Representation}
 \begin{equation}
    \mu + \Phi(L)\epsilon_t
    \label{eq:Mainf}
 \end{equation}

 \textbf{Mean of the process}
 \begin{equation}
\frac{c}{1-\phi} = \frac{2}{1-0.95} = 40
\label{eq:Mean}
\end{equation}

\textbf{Model under the Nullhypothesis}
\begin{equation}
    y_t = y_{t-1} + ut
    \label{eq:true}
\end{equation}
Whereas for the Unit root process it is important to mention that $\alpha$ should be equal to zero and our $\phi$ should be equal to one.\\

\textbf{Model under the alternative}
\begin{equation}
    y_t = \alpha + \rho y_{t-1} + ut
    \label{eq:alternative}
\end{equation}
Whereas for the stationary alternative it is important to mention that $\alpha$ should be not equal to zero and our $\phi$ should be smaller then one in absolute terms.\\

\textbf{Nullhypothesis}
\begin{equation}
\begin{aligned}
    H_0 &: \rho = 1 \, \text{(unit root)} \\
    H_1 &: \rho \neq 1 \, \text{(stationary alternative)}
\end{aligned}
\label{eq:dftest}
\end{equation}

\textbf{Test Statistic}
\begin{equation}
    t-stat: \frac{0.96-1}{0.01}
    \label{eq:dftstat2}
\end{equation}

\textbf{Conditional log likelihood contributions}
 \begin{equation}
      l_{t} =  (ln \Gamma(\frac{\nu +1}{2})- ln(\sqrt{\nu \pi} * \Gamma(\frac{\nu}{2})) - \frac{\nu+1}{2}ln(1+\frac{\epsilon_t^{2}}{\nu}))
      \label{eq:clc}
 \end{equation}

\textbf{Total conditional log likelihood}
\begin{equation}
    L_{total} = \sum_{t=2}^{T} l_t
    \label{eq:tcl}
\end{equation}

\textbf{2 Parameter specifications}
\begin{itemize}
    \item \textbf{Case (a):} $c = 2, \phi = 0.95, \theta = 0.25, \nu = 4$ (the true parameters used to generate the data),
    \item \textbf{Case (b):} $c = 1.5, \phi = 0.75, \theta = 0.5, \nu = 6$ (an alternative set of parameters).
    \label{specifica}
\end{itemize}

\textbf{Hypothesis for estimated ML}
\begin{equation}
\begin{aligned}
    H_0 &: \phi = 0.8 \, \ \\
    H_1 &: \phi \neq 0.8 \,
\end{aligned}
\label{eq:nullacht}
\end{equation}

\textbf{Test Statistic}
\begin{equation}
t = \frac{0.9452 - 0.8}{0.0100} = \frac{0.1452}{0.0100} = 14.52.
    \label{eq:tstatmlone}
\end{equation}

\textbf{Test Statistic improved}
\begin{equation}
t = \frac{0.95 - 0.8}{0.001} = \frac{0.15}{0.0100} = 150.
    \label{eq:tstatmltwo}
\end{equation}


\newpage
\section{Tables}
\textbf{OLS estimation results for Case II Dickey-Fuller}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Parameter} & \textbf{Estimate} & \textbf{Standard Error} \\ \hline
$\hat{\alpha}$     & 1.46              & 0.4                     \\ \hline
$\hat{\rho}$       & 0.96              & 0.01                    \\ \hline
\end{tabular}
\caption{Parameter estimates and their standard errors for the alternative hypothesis.}
\label{tab:parameter_estimates}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Case} & \textbf{Total Conditional Log-Likelihood} \\ \hline
a             & -1388.3248                                \\ \hline
b             & -5871.1921                                \\ \hline
\end{tabular}
\caption{Total conditional log-likelihood values for the two cases.}
\label{tab:log_likelihood_results}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Optimization Results}              & \textbf{Values}                                                                 \\ \hline
Termination Criteria                       & OPTIONS.TolX = $1.0 \times 10^{-40}$, OPTIONS.TolFun = $1.0 \times 10^{-40}$     \\ \hline
Optimized Parameters                       & 2.2456, 0.9446, 0.2834, 3.4515                                                  \\ \hline
Minimum Negative Log-Likelihood            & 1386.5                                                                          \\ \hline
Gradient at Optimum                        & $\begin{pmatrix}
0.000013 \\
-0.002566 \\
0.00002 \\
0.000001
\end{pmatrix}$                                                                  \\ \hline
Covariance Matrix of Estimates             & $\begin{pmatrix}
0.1632 & -0.0040 & 0.0030 & 0.0036 \\
-0.0040 & 0.0001 & -0.0001 & -0.0001 \\
0.0030 & -0.0001 & 0.0009 & -0.0000 \\
0.0036 & -0.0001 & -0.0000 & 0.1205 \\
\end{pmatrix}$                                                                \\ \hline
Exit Flag from fminsearch                  & 1                                                                              \\ \hline
\end{tabular}
\caption{Summary of optimization results.}
\label{tab:optimization_results}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Parameter} & \textbf{Estimate} & \textbf{Standard Error (SE)} & \textbf{95\% Confidence Interval (CI)} \\ \hline
$c$     & 2.2456 & 0.4040 & [1.4538, 3.0375] \\ \hline
$\phi$  & 0.9446 & 0.0099 & [0.9252, 0.9640] \\ \hline
$\theta$ & 0.2834 & 0.0297 & [0.2251, 0.3417] \\ \hline
$\nu$    & 3.4515 & 0.3471 & [2.7711, 4.1319] \\ \hline
\end{tabular}
\caption{Parameter estimates with standard errors and 95\% confidence intervals.}
\label{tab:firstconfidence_intervals}
\end{table}

\textbf{Results after increasing sample size}
\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Optimization Results}              & \textbf{Values}                                                                 \\ \hline
\textbf{Termination Criteria}              & OPTIONS.TolX = $1.0 \times 10^{-40}$, OPTIONS.TolFun = $1.0 \times 10^{-40}$     \\ \hline
\textbf{Optimized Parameters}              & $\hat{c} = 1.9604$, $\hat{\phi} = 0.9511$, $\hat{\theta} = 0.2425$, $\hat{\nu} = 3.9719$ \\ \hline
\textbf{Minimum Negative Log-Likelihood}   & $84302$                                                                         \\ \hline
\textbf{Gradient at Optimum}               & $\begin{pmatrix}
0.000018 \\
-0.002526 \\
0.0000 \\
-0.00003
\end{pmatrix}$                                                                  \\ \hline
\textbf{Covariance Matrix of Estimates}    & $\begin{pmatrix}
0.0023 & -0.0001 & 0.0000 & 0.0000 \\
-0.0001 & 0.0000 & -0.0000 & -0.0000 \\
0.0000 & -0.0000 & 0.0000 & -0.0000 \\
0.0000 & -0.0000 & -0.0000 & 0.0030 \\
\end{pmatrix}$                                                                \\ \hline
\textbf{Exit Flag from fminsearch}         & 1 (Optimization successfully converged)                                         \\ \hline
\end{tabular}
\caption{Summary of optimization results with an increased sample size.}
\label{tab:optimization_large_sample}
\end{table}


\textbf{Standard errors for optimized}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Parameter} & \textbf{Estimate} & \textbf{Standard Error (SE)} & \textbf{95\% Confidence Interval (CI)} \\ \hline
$c$     & 1.9604 & 0.0480 & [1.8663, 2.0545] \\ \hline
$\phi$  & 0.9511 & 0.0012 & [0.9488, 0.9534] \\ \hline
$\theta$ & 0.2425 & 0.0037 & [0.2352, 0.2498] \\ \hline
$\nu$    & 3.9719 & 0.0548 & [3.8644, 4.0794] \\ \hline
\end{tabular}
\caption{Parameter estimates with standard errors and 95\% confidence intervals for the increased sample size.}
\label{tab:confidence_intervals_large_sample}
\end{table}








\newpage
\section{Figures}
 \begin{figure}[h!]
    \centering
    \includegraphics[width=1.2\textwidth]{fina/arma_useone.jpg} 
    \caption{The Simulated ARMA(1,1)}
    \label{fig:armaone}
\end{figure}


\newpage
\section{Notizen}
\begin{itemize}
    \item Does the $\phi$ parameter tell you something about the persistence of the process
    \item Is the t test correctly interpreted?
\end{itemize}
\end{document}